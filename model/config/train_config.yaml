DIRECTORY:
    dataset: uncased #uncased, cased, nonumber

SEED:
    random_seed: 2022
    
DATALOADER:
    batch_size: 256
    shuffle: True
    drop_last: False
    
MODEL:
    model_name: KoBERT #cnn1d, KoBERT, KoELECTRA, RoBERTa
        
    pretrained_link: 
        KoBERT: kykim/bert-kor-base
        KoELECTRA: monologg/koelectra-base-v3-discriminator
        RoBERTa: klue/roberta-base
    
    num_of_classes: 19

TRAIN:
    num_of_epochs: 5
    batch_size: 256
    max_seq_len: 26         #26, 32
    learning_rate:
        pretrained: 0.00005
        cnn1d: 0.025
    dropout: 0.5
    max_grad_norm: 1
    warmup_ratio: 0.1
    loss: CrossEntropyLoss #CrossEntropyLoss, FocalLoss, WeightedCE
    optimizer: AdamW
    metric:
        - f1score
        - accuracy

LABELING:
    0: '교육'
    1: '교통/자동차'
    2: '기타소비'
    3: '대형마트'
    4: '미용'
    5: '배달'
    6: '보험'
    7: '생필품'
    8: '생활서비스'
    9: '세금/공과금'
    10: '쇼핑몰'
    11: '여행/숙박'
    12: '외식'
    13: '의료/건강'
    14: '주류/펍'
    15: '취미/여가'
    16: '카페'
    17: '통신'
    18: '편의점'
    